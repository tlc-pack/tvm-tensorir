/*
* Licensed to the Apache Software Foundation (ASF) under one
* or more contributor license agreements.  See the NOTICE file
* distributed with this work for additional information
* regarding copyright ownership.  The ASF licenses this file
* to you under the Apache License, Version 2.0 (the
* "License"); you may not use this file except in compliance
* with the License.  You may obtain a copy of the License at
*
*   http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing,
* software distributed under the License is distributed on an
* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
* KIND, either express or implied.  See the License for the
* specific language governing permissions and limitations
* under the License.
*/
#include <unordered_map>

#include "../../tir/schedule/analysis.h"
#include "../utils.h"

namespace tvm {
namespace tir {
/*!
* \brief Get the buffer dimensions for all the read buffers of a block, but marks the reduction
* buffers' dimensions as -1
* \param block_sref The block to be processed
* \return The buffer dimensions for all the read buffers of a block, except for reduction buffers
* \note The method is not designed for generic analysis and relies on assumptions in the scenario
* of multi-level tiling, so it's intentionally kept inside this file not in the analysis header
*/
std::vector<int> GetReadBufferNDims(const StmtSRef& block_sref);
}  // namespace tir
}  // namespace tvm

namespace tvm {
namespace meta_schedule {

using tir::BlockRV;
using tir::ExprRV;
using tir::IterVarType;
using tir::LoopRV;
using tir::Schedule;

/*!
* \brief Configuration of data reuse type:
* 0) kNoReuse: no reuse is allowed, then no cache_read/write is performed.
* 1) kMayReuse: reuse is allowed, but no reuse is explored.
* 2) kMustReuse: reuse is allowed and no reuse is not explored.
*/
enum class ReuseType : int32_t {
 kNoReuse = 0,
 kMayReuse = 1,
 kMustReuse = 2,
};

/*!
* \brief Converts a string to ReuseType.
* \param str The string to be converted.
* \return The converted ReuseType.
*/
ReuseType Str2ReuseType(const String& str);

/*! \brief Configuration of data reuse patterns */
struct ReuseConfigAutoMovement {
 /*! \brief Type of data reuse: no-reuse, may-reuse or must-reuse */
 ReuseType req;
 /*! \brief Which levels are caching stage inserted at */
 std::vector<std::vector<int>> levels;
 /*! \brief The storage scope */
 std::vector<String> scope;

 /*! \brief Default constructor: no data reuse */
 ReuseConfigAutoMovement() : req(ReuseType::kNoReuse) {}

 /*! \brief Construct from a configuration dictionary */
 explicit ReuseConfigAutoMovement(const Map<String, ObjectRef>& config)
     : req(Str2ReuseType(Downcast<String>(config.at("req")))),
       scope(support::AsVector<String, String>(Downcast<Array<String>>(config.at("scope")))) {
   ICHECK_EQ(config.size(), 3);
   auto level = Downcast<Array<Array<Integer>>>(config.at("levels"));
   for (const auto& l : level) {
     levels.push_back(support::AsVector<Integer, int>(l));
   }
 }
};

/*! \brief The state of auto scheduling for the multi-level tiling rule */
struct StateAutoMovement {
 /*! \brief The schedule to date */
 Schedule sch;
 /*! \brief The block to be tiled */
 BlockRV block_rv;
 /*! \brief The write cache */
 Optional<BlockRV> write_cache;
 /*! \brief Indicating if the write cache is generated by cache_write */
 bool write_cache_is_added;
 /*! \brief The loop tiles */
 Array<Array<LoopRV>> tiles;
 /*! \brief Whether tensor core is used for the inner computation */
 bool tensor_core_is_used = false;

 /*! \brief Default constructor */
 explicit StateAutoMovement(Schedule sch, BlockRV block_rv, Optional<BlockRV> write_cache = NullOpt,
                bool write_cache_is_added = false, Array<Array<LoopRV>> tiles = {})
     : sch(sch),
       block_rv(block_rv),
       write_cache(write_cache),
       write_cache_is_added(write_cache_is_added),
       tiles(tiles){}
};

/*!
* \brief Helper to apply a sub-rule to a list of auto scheduling states
* \tparam FLambda The type of the sub-rule functor
* \param states The list of states to be applied
* \return The list of states after applying the sub-rule
*/
template <class FLambda>
std::vector<StateAutoMovement> SubRule(std::vector<StateAutoMovement> states, FLambda sub_rule) {
 std::vector<StateAutoMovement> results;
 for (auto&& state : states) {
   std::vector<StateAutoMovement> next = sub_rule(std::move(state));
   results.insert(results.end(),
                  std::make_move_iterator(next.begin()),  //
                  std::make_move_iterator(next.end()));
 }
 return results;
}

/*!
* \brief The mega rule: multi-level tiling with data reuse
*/
class MultiLevelTilingAutoMovementNode : public ScheduleRuleNode {
public:
 // SubRule 1. add write cache
 inline std::vector<StateAutoMovement> AddWriteReuse(StateAutoMovement state) const;
 // SubRule 2. tile the loop nest
 inline std::vector<StateAutoMovement> TileLoopNest(StateAutoMovement state) const;
 // SubRule 3. add read cache
 inline std::vector<StateAutoMovement> AddReadReuse(StateAutoMovement state) const;
 // SubRule 4. fuse write cache
 inline std::vector<StateAutoMovement> FuseWriteReuse(StateAutoMovement state) const;
 // SubRule 5. detect tensor core
 inline std::vector<StateAutoMovement> DetectTensorCore(StateAutoMovement state) const;
     // Do nothing; Inherited from ScheduleRuleNode
 void InitializeWithTuneContext(const TuneContext& context) final {}
 // Entry of the mega rule; Inherited from ScheduleRuleNode
 Array<Schedule> Apply(const Schedule& sch, const BlockRV& block_rv) final {
   if (!NeedsMultiLevelTiling(sch->state(), sch->GetSRef(block_rv))) {
     return {sch};
   }
   std::vector<StateAutoMovement> states{StateAutoMovement(sch, block_rv)};
   states = SubRule(std::move(states), [&](StateAutoMovement state) { return DetectTensorCore(state); });
   states = SubRule(std::move(states), [&](StateAutoMovement state) { return TileLoopNest(state); });
   states = SubRule(std::move(states), [&](StateAutoMovement state) { return AddWriteReuse(state); });
   states = SubRule(std::move(states), [&](StateAutoMovement state) { return AddReadReuse(state); });
//   states = SubRule(std::move(states), [&](StateAutoMovement state) { return FuseWriteReuse(state); });
   Array<Schedule> results;
   for (auto&& state : states) {
     results.push_back(std::move(state.sch));
   }
   return results;
 }

public:
 /*!
  * \brief The tiling structure. Recommended:
  * - 'SSRSRS' on CPU
  * - 'SSSRRSRS' on GPU
  */
 String structure;
 /*! \brief For each level of tiles, which thread axis it is bound to */
 Array<String> tile_binds;
 /*! \brief The maximum size of the innermost factor */
 int max_innermost_factor;
 /*! \brief The length of vector lane in vectorized cooperative fetching */
 int vector_load_max_len;
 /*! \brief Data reuse configuration for reading */
 ReuseConfigAutoMovement reuse_read_;
 /*! \brief Data reuse configuration for writing */
 ReuseConfigAutoMovement reuse_write_;
 /*! \brief The indices of spatial tiles in `structure` */
 std::vector<int> s_indices_;
 /*! \brief The indices of reduction tiles in `structure` */
 std::vector<int> r_indices_;
 /*! \brief The tensor intrinsinc for doing computation */
 String compute_intrin;
 

 void VisitAttrs(tvm::AttrVisitor* v) {
   v->Visit("structure", &structure);
   v->Visit("tile_binds", &tile_binds);
   v->Visit("max_innermost_factor", &max_innermost_factor);
   v->Visit("vector_load_max_len", &vector_load_max_len);
   // `reuse_read_` is not visited
   // `reuse_write_` is not visited
   // `s_indices_` is not visited
   // `r_indices_` is not visited
 }

 static constexpr const char* _type_key = "meta_schedule.MultiLevelTilingAutoMovement";
 TVM_DECLARE_FINAL_OBJECT_INFO(MultiLevelTilingAutoMovementNode, ScheduleRuleNode);
};

inline std::vector<StateAutoMovement> MultiLevelTilingAutoMovementNode::AddWriteReuse(StateAutoMovement state) const {
 const ReuseConfigAutoMovement& config = this->reuse_write_;
 if (config.req == ReuseType::kNoReuse) {
   return {std::move(state)};
 }
 // Case 1. If the write cache is already there, we don't need to add another.
 if (config.req == ReuseType::kMayReuse) {
   Array<BlockRV> consumer_rvs = state.sch->GetConsumers(state.block_rv);
   if (consumer_rvs.size() == 1 && IsWriteCache(state.sch->GetSRef(consumer_rvs[0]))) {
     state.write_cache = consumer_rvs[0];
     state.write_cache_is_added = false;
     return {std::move(state)};
   }
 }
 std::vector<StateAutoMovement> results;
 results.reserve(2);
 // Case 2. No write cache is added
 if (config.req == ReuseType::kMayReuse) {
   StateAutoMovement new_state(/*sch=*/state.sch->Copy(), /*block_rv=*/state.block_rv,
                   /*write_cache=*/NullOpt,
                   /*write_cache_is_added=*/false);
   new_state.sch->Seed(state.sch->ForkSeed());
   results.emplace_back(std::move(new_state));
 }
 // Case 3. Add one write cache
 
 for (const auto& level : config.levels) {
   Schedule sch = state.sch->Copy();
   sch->Seed(state.sch->ForkSeed());
   BlockRV block_rv = state.block_rv;
   if (state.tensor_core_is_used) {
     const LoopRV& loop_rv = state.tiles[r_indices_.front()-1].back();
     block_rv = sch->WriteAt(loop_rv, block_rv, 0, "wmma.accumulator", false);
   } else {
     for (int i = 0; i < level.size(); i++) {
       const LoopRV& loop_rv = state.tiles[level[i] - 1].back();
       block_rv = sch->WriteAt(loop_rv, block_rv, 0, config.scope[i], false);
     }
   }
   StateAutoMovement new_state = state;
   new_state.sch = sch;
   results.push_back(std::move(new_state));
 }
 return results;
 
}

inline std::vector<StateAutoMovement> MultiLevelTilingAutoMovementNode::TileLoopNest(StateAutoMovement state) const {
 Schedule& sch = state.sch;
 const BlockRV& block_rv = state.block_rv;
 // Step 1. Assuming trivial binding, pair the loops and their iter-var-types
 Array<LoopRV> loops = sch->GetLoops(block_rv);
 std::vector<IterVarType> iter_types = GetBlockVarTypes(sch->GetSRef(state.block_rv));
 ICHECK_EQ(loops.size(), iter_types.size());
 // Step 2. For each loop axis, tile it
 std::vector<Array<LoopRV>> tiles(s_indices_.size() + r_indices_.size());
 for (int i = 0, n = loops.size(); i < n; ++i) {
   const std::vector<int>* idx = nullptr;
   if (iter_types[i] == IterVarType::kDataPar) {
     idx = &s_indices_;
   } else if (iter_types[i] == IterVarType::kCommReduce) {
     idx = &r_indices_;
   } else {
     continue;
   }
   // Do the split
   int n_tiles = idx->size();
   LoopRV loop = loops[i];
   Array<ExprRV> factors = sch->SamplePerfectTile(
       /*loop=*/loop,
       /*n=*/n_tiles,
       /*max_innermost_factor=*/max_innermost_factor);
   Array<LoopRV> splits = sch->Split(/*loop=*/loop,
                                     /*factors=*/{factors.begin(), factors.end()});
   // Put every tile to its slot
   for (int j = 0; j < n_tiles; ++j) {
     tiles[idx->at(j)].push_back(splits[j]);
   }
 }
 // Step 3. Reorder to organize the tiles
 sch->Reorder(support::ConcatArrayList<LoopRV>(tiles.begin(), tiles.end()));
 sch->Annotate(tiles[r_indices_.front()].back(), tir::attr::pipeline_scope, Integer(2));
 sch->Annotate(tiles[r_indices_.back()].back(), tir::attr::pipeline_scope, Integer(2));
 // Step 4. Bind the tiles to threads
 int n_binds = std::min(tile_binds.size(), tiles.size());
 for (int i = 0; i < n_binds; ++i) {
   LoopRV fused = sch->Fuse(tiles[i]);
   sch->Bind(fused, tile_binds[i]);
   tiles[i] = {fused};
 }
 state.tiles = Array<Array<LoopRV>>{tiles.begin(), tiles.end()};
 return {state};
}

inline std::vector<StateAutoMovement> MultiLevelTilingAutoMovementNode::AddReadReuse(StateAutoMovement state) const {
 const ReuseConfigAutoMovement& config = this->reuse_read_;
 if (config.req == ReuseType::kNoReuse) {
   return {std::move(state)};
 }
 ICHECK(config.req != ReuseType::kMayReuse);
 std::vector<StateAutoMovement> results;
 results.reserve(config.levels.size());
 for (const std::vector<int>& level : config.levels) {
   Schedule sch = state.sch->Copy();
   sch->Seed(state.sch->ForkSeed());
   // Enumerate all buffers that are read but not written
   
   std::vector<int> read_buffer_ndims = tir::GetReadBufferNDims(sch->GetSRef(state.block_rv));
   for (int i = 0, n_reads = read_buffer_ndims.size(); i < n_reads; ++i) {
     BlockRV block_rv = state.block_rv;
     int buffer_ndim = read_buffer_ndims[i];
     if (buffer_ndim == -1) {
       continue;
     }
     bool has_read_at = false;
     if (state.tensor_core_is_used) {
       const LoopRV& loop_rv = state.tiles[r_indices_.back()].back();
       block_rv = sch->ReadAt(loop_rv, block_rv, i, i==1? "wmma.matrix_a": "wmma.matrix_b", false);
       has_read_at = true;
     }
     for (int j = 0; j < level.size(); j++) {
       int level_for_scope = level[j];
       String scope = config.scope[j];
       const LoopRV& loop_rv = state.tiles[level_for_scope - 1].back();
       // Do read_at
       block_rv = sch->ReadAt(loop_rv, block_rv, has_read_at?0:i, scope, false);
       has_read_at=true;
       if (runtime::StorageScope::Create(scope).rank == runtime::StorageRank::kShared) {
         sch->Annotate(block_rv, "pragma_double_buffer", Integer(1));
       }
       // todo:(jhy) do we need to annotate here?
       //     {
       //       tir::Annotate(sch->state(), sch->GetSRef(cache_read_block),  //
       //                     tir::attr::meta_schedule_cache_type,
       //                     Integer(tir::attr::meta_schedule_cache_type_read));
       //     }
       
     }
   }
   StateAutoMovement new_state = state;
   new_state.sch = sch;
   results.push_back(std::move(new_state));
 }
 return results;
}

inline std::vector<StateAutoMovement> MultiLevelTilingAutoMovementNode::FuseWriteReuse(StateAutoMovement state) const {
 const ReuseConfigAutoMovement& config = this->reuse_write_;
 if (config.req == ReuseType::kNoReuse) {
   return {std::move(state)};
 }
 // If the only-consumer does not exist, or is not elementwise, then do not do fusion
 if (!state.write_cache.defined() || state.tensor_core_is_used) {
   return {std::move(state)};
 }
 std::vector<StateAutoMovement> results;
 // Special case.
 //    Stages added by `cache_write` must be fused at some level, otherwise it has no benefit.
 //    On the other hand, If the consumer stage is not added by  `cache_write`,
 //    we may choose not to fuse by setting `must_cache_write = False`
 if (!state.write_cache_is_added && config.req != ReuseType::kMustReuse) {
   results.push_back(state);
 }
 BlockRV consumer = state.write_cache.value();
 // Enumerate the level of tile to be fused at
 for (int level : config.levels[0]) {
   Schedule sch = state.sch->Copy();
   sch->Seed(state.sch->ForkSeed());
   const LoopRV& loop_rv = state.tiles[level - 1].back();
   sch->ReverseComputeAt(consumer, loop_rv, true);
   StateAutoMovement new_state = state;
   new_state.sch = sch;
   results.push_back(std::move(new_state));
 }
 return results;
}

std::vector<StateAutoMovement> MultiLevelTilingAutoMovementNode::DetectTensorCore(StateAutoMovement state) const {
  if (compute_intrin.empty()) {
    return {state};
  }
  std::vector<StateAutoMovement> result;
//  result.push_back(state);
  Schedule sch = state.sch->Copy();
  sch->Seed(state.sch->ForkSeed());
  state.sch = sch;
  Optional<tir::TensorizeInfo> opt_tensorize_info =
      GetTensorizeLoopMapping(state.sch->state(), state.sch->GetSRef(state.block_rv),
                              tir::TensorIntrin::Get(compute_intrin)->description);
  if (!opt_tensorize_info) {
    return result;
  }
  const tir::TensorizeInfoNode* info = opt_tensorize_info.value().get();
  BlockRV block_rv = state.block_rv;
  // Construct a mapping from tir loops back to LoopRVs
  Map<tir::StmtSRef, LoopRV> loop2rv;
  {
    Array<LoopRV> loop_rvs = state.sch->GetLoops(block_rv);
    for (const LoopRV& loop_rv : loop_rvs) {
      loop2rv.Set(state.sch->GetSRef(loop_rv), loop_rv);
    }
  }
  // Split the loops
  arith::Analyzer analyzer;
  std::unordered_set<const tir::StmtSRefNode*> inner_loops;
  std::vector<LoopRV> reorder_suffix;
  reorder_suffix.resize(info->loop_map.size());
  for (const auto& kv : info->loop_map) {
    // Extract mapping (block_loop => desc_loop)
    const tir::StmtSRef& block_loop_sref = kv.first;
    const tir::ForNode* block_loop = block_loop_sref->StmtAs<tir::ForNode>();
    const tir::ForNode* desc_loop = kv.second.get();
    ICHECK(block_loop != nullptr && desc_loop != nullptr);
    // Extract the loop extent
    PrimExpr block_extent = analyzer.Simplify(block_loop->extent);
    PrimExpr desc_extent = analyzer.Simplify(desc_loop->extent);
    const auto* int_block_extent = block_extent.as<IntImmNode>();
    const auto* int_desc_extent = desc_extent.as<IntImmNode>();
    ICHECK(int_block_extent != nullptr && int_desc_extent != nullptr);
    // Check divisibility
    int64_t total = int_block_extent->value;
    int64_t inner = int_desc_extent->value;
    ICHECK_EQ(total % inner, 0);
    int64_t outer = int_block_extent->value / int_desc_extent->value;
    // Do the split
    Array<LoopRV> split =
        state.sch->Split(loop2rv.at(block_loop_sref), {Integer(outer), Integer(inner)});
    ICHECK_EQ(split.size(), 2);
    inner_loops.insert(state.sch->GetSRef(split[1]).operator->());
    // The inner split will be reordered to the loop domain that is tensorized
    int desc_loop_index = info->desc_loop_indexer.at(GetRef<tir::For>(desc_loop));
    reorder_suffix[desc_loop_index] = split[1];
  }
  // Reorder the loops
  std::vector<LoopRV> reorder_list;
  bool meet = false;
  Array<LoopRV> all_loops = state.sch->GetLoops(block_rv);
  for (const LoopRV& loop : all_loops) {
    if (inner_loops.count(state.sch->GetSRef(loop).operator->())) {
      meet = true;
    } else if (meet) {
      reorder_list.push_back(loop);
    }
  }
  reorder_list.insert(reorder_list.end(), reorder_suffix.begin(), reorder_suffix.end());
  state.sch->Reorder(reorder_list);
  // Do blockize
  if (!reorder_suffix.empty()) {
    state.block_rv = state.sch->Blockize(reorder_suffix[0]);
  }
  // Annotate the block
  //todo:why?
  state.sch->Annotate(block_rv, tir::attr::auto_tensor_core, String("0"));
  state.sch->Annotate(state.block_rv, tir::attr::auto_tensor_core, String("4"));
  tir::BlockRV root_rv = state.sch->GetBlock("root");
  state.sch->Annotate(root_rv, "warp_execution", Bool(true));
  state.tensor_core_is_used = true;
  result.push_back(state);
  return result;
}

// Constructor

ScheduleRule ScheduleRule::MultiLevelTilingAutoMovement(String structure, Optional<Array<String>> tile_binds,
                                           Optional<Integer> max_innermost_factor,
                                           Optional<Integer> vector_load_max_len,
                                           Optional<Map<String, ObjectRef>> reuse_read,
                                           Optional<Map<String, ObjectRef>> reuse_write,
                                                        Optional<String> compute_intrin) {
 ObjectPtr<MultiLevelTilingAutoMovementNode> n = make_object<MultiLevelTilingAutoMovementNode>();
 n->structure = structure;
 n->tile_binds = tile_binds.value_or({});
 n->max_innermost_factor = max_innermost_factor.value_or(Integer(-1))->value;
 n->vector_load_max_len = vector_load_max_len.value_or(Integer(-1))->value;
 n->reuse_read_ = reuse_read.defined() ? ReuseConfigAutoMovement(reuse_read.value()) : ReuseConfigAutoMovement();
 n->reuse_write_ = reuse_write.defined() ? ReuseConfigAutoMovement(reuse_write.value()) : ReuseConfigAutoMovement();
 n->compute_intrin=compute_intrin.value_or("");
 for (int i = 0, len = structure.size(); i < len; ++i) {
   char c = structure.data()[i];
   if (c == 'S') {
     n->s_indices_.push_back(i);
   } else if (c == 'R') {
     n->r_indices_.push_back(i);
   } else {
     LOG(FATAL) << "ValueError: Invalid tiling structure: " << structure;
   }
 }
 return ScheduleRule(n);
}

TVM_REGISTER_NODE_TYPE(MultiLevelTilingAutoMovementNode);
TVM_REGISTER_GLOBAL("meta_schedule.ScheduleRuleMultiLevelTilingAutoMovement")
   .set_body_typed(ScheduleRule::MultiLevelTilingAutoMovement);

}  // namespace meta_schedule
}  // namespace tvm
